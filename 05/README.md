# 5.1 神经元模型
* M-P神经元模型
    * 输入x(i)，连接权重w(i)
    * 与阀值比较，输出y = SUM(i=1,i<=n,w(i) * x(i) - z)
* 阶跃函数 不连续，不光滑，所以用Sigmoid作激活函数

# 5.2 感知机与多层网络
* 单一感知机，可解决，与、或、非运算，不能解决异或
* 神经网络学的就是，连接权和阀值

# 5.3 误差逆传播算法
* error BackPropagation，简称 BP
* 基于累计误差最小话更新规则：读取完整个训练集D后，才更新参数，更新频率低
* 基于标准误差最小话更新规则：单个样例就更新参数，更新频繁高，适用训练集D非常大时
* 缓解过拟合策略
    * 早停：发现训练集误差降低但验证集误差升高，停止
    * 正则化：误差函数中，添加网络复杂度的表示

# 5.4 全局最小与局部极小
* 跳出局部极小策略
    * 多组不同参数初始化多个神经网络同时训练
    * 迭代时，接受次优解
    * 随机梯度下降：相比标注梯度下降法，加入随机因素
    * 遗传算法

# 5.5 其他常见神经网络
* RBF：使用径向基函数作为隐层
* ART：竞争型学习，常用的无监督学习策略
    * 竞争最简单方式：与每个神经元的代表向量计算距离(dis，主动学习，相似性)
* SOM网络：自组织映射，竞争学习型的无监督神经网络
* 级联相关网络：不仅学习连接权/阀值，连网络结构也学习
    * 2个主要成分：级联、相关
    * 相比前馈神经网络
        * 无需设置网络层数、隐层数目
        * 训练快
        * 但数据较小时，易过拟合
* Elman网络
    * 递归神经网络
    * 允许网络中出现环形
* Boltzmann机
    * 一种基于能量的模型，训练就是min能量函数
    * 神经元2层
        * 显层：数据输入/输出
        * 隐层：数据的内在表达
    * 神经元都是bool

# 5.6 深度学习
* 参数越多，模型复杂度越高，能完成更复杂的学习任务
* 提高容量的简单办法：增加隐层数量
* 无监督逐层训练
    * 上一层的输出，作为下一层的输入
    * 全部完成后，再对整个网络进行fine-tuning
* 权共享
    * 一组神经元，使用相同连接权
    * CNN
* 通过多层处理，将初始的“低层”特征转化为“高层”特征