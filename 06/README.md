# 6.1 间隔与支持向量
* 通过线性方程划分超平面 w**T * x + b = 0
* 样本中任意点x，到超平面(w,b) 的距离为：r = abs(w**T * x + b) / ||w||
* (w, b)能将训练样本正确分类，则
    * w**T * x + b >= +1, y = +1
    * w**T * x + b <= -1, y = -1
* 距离超平面最近的几个样本点，被称为“支持向量”
* 两个异类支持向量道超平面的距离之和为：r = 2/||w||，称为“间隔”
* 找最大间隔
```
max(w, b) 2/||w||
=> max(||w||**-1)
=> min(||w||**2)
=> min(w,b) 1/2 * ||w||**2
```
# 6.2 对偶问题
* 使用拉格朗日乘子法
```
L(w, b, a) = 1/2 * ||w|| ** 2 + SUM(i=1,i<=m, a(1-y(w**T*x + b) ) )
# 对w和b的偏道为零可得
w = SUM(i=1, i<=m, a * y * x )
0 = SUM(i=1, i<=m, a * y)
```
* 训练完成后，大部分训练样本都不需要保留，最终模型仅与支持向量有关

# 6.3 核函数
* 现实中异或问题，就不能线形可分
* 映射到更高维再划分
* 核函数k(xi,xj): 表示xi与xj在特征空间的内积等于它们在原始样本空间中通过函数k计算的结果
* 核函数选择，称为支持向量机的最大变数

# 6.4 软间隔与正则化
* 硬间隔：所有样本都划分正确
* 软间隔：允许某些样本不满足
* 最大化间隔损失函数，非凸非连续，所以常用替代损失函数
    * hinge(z) = max(0, 1-z)
    * 指数损失(z) = exp(-z)
    * 对率损失(z) = log(1 + exp(-z))
* 每个样本都有一个松弛变量，表征该样本不满足约束的程度
* 软间隔支持向量机最终模型仅与支持向量有关，可以通过hinge损失函数保持稀疏性

# 6.5 支持向量回归(Support Vector Regression, SVR)
* 传统回归，f(x)与y完全相同，loss=0
* SVR，f(x)与y之间的差别绝对值小于e，loss=0
* 相当于f(x)，构建宽度为2e的区间内，都算正确

# 6.6 核方法
* 核方法：基于核函数的学习方法
* 通过核化，将线性学习器拓展为非线性学习器